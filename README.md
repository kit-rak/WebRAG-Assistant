# ğŸ” Multimodal RAG Assistant

A Streamlit-based application that enables users to query and summarize **PDFs** and **webpages** using a multimodal Retrieval-Augmented Generation (RAG) pipeline. It integrates visual and text understanding powered by **ColiVara**, **Firecrawl**, and **Janus**.

## âœ¨ Features

* ğŸŒ Scrape and analyze webpages via Firecrawl.
* ğŸ“„ Upload PDFs for intelligent processing.
* ğŸ¤– Ask questions and receive answers based on document visuals and content.
* ğŸ§  Automatic summarization of documents.
* ğŸŒ Multilingual support (English, Spanish, French, German, Chinese).

## ğŸ§± Tech Stack

* **Streamlit** â€“ Interactive UI
* **Firecrawl** â€“ Web scraping and screenshots
* **ColiVara** â€“ Vector search and document indexing
* **Janus** â€“ Multimodal vision-language transformer
* **FPDF** â€“ PDF generation
* **Pillow (PIL)** â€“ Image processing

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/kit-rak/WebRAG-Assistant.git
cd WebRAG-Assistant
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Add Environment Variables

Create a `.env` file in the root directory:

```env
FIRECRAWL_API_KEY=your_firecrawl_api_key
COLIVARA_API_KEY=your_colivara_api_key
```

### 4. Launch the App

```bash
streamlit run main.py
```
## ğŸ§  Installing DeepSeek Janus
To integrate the Janus multimodal model into your project, follow these steps:

### 1. Clone the Janus Repository

```bash
git clone https://github.com/deepseek-ai/Janus.git
cd Janus
```

### 2. Install Dependencies

```bash
pip install -e .
```

This command installs the Janus package in editable mode, allowing for easy development and updates.

### 3. (Optional) Install Gradio for Demo Interface
If you wish to explore the demo interface provided by Janus:

```bash
pip install -e .[gradio]
```

### 4. Launch the Janus Demo App
To run the demo application:

```bash
python demo/app_januspro.py
```

Access the application at http://localhost:7860.

Note: Ensure that your system meets the necessary requirements for running DeepSeek Janus efficiently. For optimal performance, a CUDA-capable GPU is recommended.

## ğŸ§  How It Works

1. **Input**: User submits a URL or PDF file.
2. **Firecrawl** (for URLs): Takes a full-page screenshot and returns it.
3. **PDF Generation**: Screenshot is sliced and converted into a PDF.
4. **Indexing**: Content is indexed using ColiVara's vector database.
5. **Querying**: Janus LLM receives the visual context and responds intelligently.
6. **Chat**: Users interact via chat or generate summaries.

## ğŸ—€ Interface Preview

Hereâ€™s how the app looks in action:

![App Screenshot](/assets/WebRAG.png)

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py            # Streamlit app UI
â”œâ”€â”€ rag_code.py        # RAG + Janus integration logic
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ .env               # Environment variable file (not committed)
```

## ğŸ“Œ Notes

* Ensure that GPU support is available for Janus LLM inference.
* Image-rich documents yield the best results with this multimodal approach.
* PDFs are visually parsed â€“ not just based on raw text.

## ğŸ“œ License

This project is licensed under the MIT License.

## ğŸ“¬ Contact
- **LinkedIn:** [kit-rak](https://www.linkedin.com/in/kit-rak)
- **GitHub:** [kit-rak](https://github.com/kit-rak)

ğŸš€ **Happy Analyzing!**
